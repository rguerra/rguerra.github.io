<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="rguerra" />


<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/textmate.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="_resources/css/styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>

<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.0/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
  padding-left: 10px;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">My Personal Website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="probability" class="section level2">
<h2>Probability</h2>
<div id="probability-concepts" class="section level3">
<h3>Probability Concepts <!--{{{--></h3>
<ul>
<li><p><strong>Probability Distribution</strong>: which describes the probabilities of something occurring over the range of possible feature values.</p></li>
<li><p><strong>Central Limit Theorem</strong>: which says that lots of small random numbers will add up to something Gaussian.</p></li>
<li><strong>Conditional Probability</strong>: is a measure of the probability of an event given that (by assumption, presumption, assertion or evidence) another event has occurred <span class="citation">(Wikipedia)</span>.
<ul>
<li><strong>Two Variables</strong>: <span class="math">\[P(A,B) = P(A)P(B|A)\]</span></li>
<li><strong>Three Variables</strong>: <span class="math">\[P(U) = P(A,B) = P(A)P(B|A)\]</span> <span class="math">\[P(U,C) = P(U)P(C|U)\]</span> <span class="math">\[P(A,B,C) = P(A)P(B|A)P(C|A,B)\]</span></li>
<li><strong>More than three</strong>: <span class="math">\[P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\]</span></li>
</ul></li>
<li><p><strong>Chain Rule</strong>: permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities. The rule is useful in the study of Bayesian networks, which describe a probability distribution in terms of conditional probabilities <span class="citation">(<em>Chain Rule (Probability)</em>)</span>. <span class="math">\[P(x_1, x_2, x_3,...,x_n)    = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1,...,x_{n‐1})\]</span></p></li>
<li><p><strong>Markov Property</strong>: A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it <span class="citation">(“Markov Property”)</span>.</p></li>
<li><p><strong>Markov Assumption</strong>: is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model <span class="citation">(“Markov Property”)</span>.</p></li>
<li><p><strong>Maximum Likelihood Estimation</strong>: probability of observing the given data as a function of the parameters <span class="math">\(\theta\)</span> <span class="citation">(<em>Maximum Likelihood Estimation</em>)</span>.</p></li>
</ul>
<!--}}}-->
</div>
</div>
<div id="statistics" class="section level2">
<h2>Statistics</h2>
<div id="statistics-concepts" class="section level3">
<h3>Statistics Concepts <!--{{{--></h3>
<ul>
<li><p><strong>Regression</strong>: you want to predict one set of numbers given another set of numbers. You’ll also sometimes hear people refer to the set of numbers that are the inputs as predictors or features.</p></li>
<li><p><strong>Correlation</strong>: is just a measure of how well linear regression could be used to model the relationship between two variables.</p></li>
<li><p><strong>Mean Square Error</strong>: this is a way to combine all the squared errors from the points in the dataaset.</p></li>
<li><p><strong>Expectation</strong>: The expected value can be viewed as a weighted average. The expected value of a random variable is the average of all values it can take; thus the expected value is what one expects to happen on average.If the outcomes <span class="math">\(x_i\)</span> are not equally probable, then the simple average must be replaced with the weighted average, which takes into account the fact that some outcomes are more likely than the others. The intuition however remains the same: the expected value of <span class="math">\(X\)</span> is what one expects to happen on average.</p></li>
<li><p><strong>Variance</strong>: of the set of numbers is a measure of how spread out the values are. It is computed as the sum of the squared distances between each element in the set and the expected value of the set (the mean, µ. The variance looks at the variation in one variable compared to its mean.</p></li>
<li><p><strong>Covariance</strong>: is a generalization of variance instead of looking at the variation of one variable with respect to its mean it looks at how two variable vary together. The variance looks at the variation in one variable compared to its mean.</p></li>
<li><strong>Covariance Matrix</strong>: The covariance can be used to look at the correlation between all pairs of variables within a set of data. We need to compute the covariance of each pair, and these are then put together into what is imaginatively known as the covariance matrix.
<ul>
<li><em>NOTE</em>: that the matrix is square, that the elements on the leading diagonal of the matrix are equal to the variances, and that it is symmetric since <span class="math">\(cov(x_i , x_j ) = cov(x_j , x_i)\)</span>.</li>
</ul></li>
<li><strong>Multinomial Distribution</strong>: it models the probability of counts for rolling a <span class="math">\(k\)</span> sided die n times. For <span class="math">\(n\)</span> independent trials each of which leads to a success for exactly one of <span class="math">\(k\)</span> categories, with each category having a given fixed success probability (these are the parameters of the distribution), the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.
<ul>
<li><p>The simpler clase is the <strong>binomial distribution</strong> where instead of a <span class="math">\(k\)</span> sided die it is a two sided coin.</p></li>
<li><p><strong>Parameters</strong>: are <span class="math">\(n\)</span> (the number of trials that the die was tossed) and <span class="math">\(p\)</span>, where <span class="math">\(p = (p_1, ..., p_k)\)</span> these are probabilities <span class="math">\(p_1, ..., p_k\)</span> corresponding to the <span class="math">\(k\)</span> possible mutually exclusive outcomes.</p></li>
</ul></li>
<li><p><strong>Multivariate Normal Distribution</strong>: the multivariate normal distribution or multivariate Gaussian distribution, is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions. One possible definition is that a random vector is said to be <span class="math">\(k\)</span>-variate normally distributed if every linear combination of its <span class="math">\(k\)</span> components has a univariate normal distribution. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value. <!--}}}--></p></li>
</ul>
</div>
</div>
<div id="machine-learning" class="section level2">
<h2>Machine Learning</h2>
<div id="machine-learning-concepts" class="section level3">
<h3>Machine Learning Concepts: <!--{{{--></h3>
<ul>
<li><p><strong>False Positive</strong> and <strong>False Negative</strong>: To understand this think of it as a binary classification problem in which the classifier will output either <em>positive</em> or <em>negative</em>. Now think of the term <strong>False Positive</strong> as a compound term that combines the gold standard answer with your classifier’s answer. Just divided the term into two [Gold Standard] [Classifier’s Answer]. If the classifier says that an instance is <em>positive</em> but it is <em>wrong</em> then the error is a [False] [Positive]. If the classifier says that an instance is <em>negative</em> but it is <em>wrong</em> then the error is a [False] [Negative].</p></li>
<li><p><strong>Bias-Variance Dilemma</strong>: more complex models do not necessarily result in better results. more complicated models have inherent dangers such as overfitting, and requiring more training data.More complex classifiers will tend to improve the bias, but the cost of this is higher variance, while making the model more specific by reducing the variance will increase the bias.</p>
<ul>
<li><p><strong>Bias</strong>: in a model means it is not accurate and doesn’t match the data well,</p></li>
<li><p><strong>Variance</strong>: in a model means that it is not very precise and there is a lot of variation in the results.</p></li>
</ul></li>
<li><p><strong>Decision Boundary or Discriminant Function</strong>: a straight line (in 2D, a plane in 3D, and a hyperplane in higher dimensions) where the neuron fires or the datapoints belong to class A on one side of the line, and doesn’t or belong to class B on the other.</p></li>
<li><p><strong>Margin</strong>: is defined as the distance between the separating hyperplane (decision boundary) and the training samples that are closest to this hyperplane, which are the so-called support vectors in a <strong>Support Vector Machine</strong>.</p></li>
<li><p><strong>Local Markov Assumption</strong>: A variable X is independent of its non-descendants given its parents (and only its parents)</p></li>
<li><p><strong>Gradient</strong>: is a generalization of the usual concept of derivative of a function in one dimension to a function in several dimensions <span class="math">\(f(x_1, ..., x_n)\)</span> <span class="citation">(<em>Gradient</em> 2016)</span>.</p></li>
<li><strong>Gradient Descent</strong>: update a set of parameters in an iterative manner to minimize an error function <span class="citation">(“What’s the Difference Between Gradient Descent and Stochastic Gradient Descent?” 2016)</span>.
<ul>
<li><strong>Gradient Descent</strong>: you have to run through <strong>ALL</strong> the samples in your training set to do a single update for a parameter in a particular iteration <span class="citation">(“What’s the Difference Between Gradient Descent and Stochastic Gradient Descent?” 2016)</span>.</li>
<li><strong>Stochastic Gradient Descent</strong>: you use <strong>ONLY ONE</strong> training sample from your training set to do the update for a parameter in a particular iteration <span class="citation">(“What’s the Difference Between Gradient Descent and Stochastic Gradient Descent?” 2016)</span>.</li>
</ul></li>
<li><strong>Soft-Max</strong>:
<ul>
<li><p>The more technical definition is: “Probability of choosing <span class="math">\(1\)</span> to <span class="math">\(N\)</span> discrete items. Mapping from vector space to a multinomial over words” <span class="citation">(Moody 2016)</span>.</p></li>
<li>The less technical: “The machine learning softmax is used for classification. Suppose you have <span class="math">\(n\)</span> classes. For any given feature <span class="math">\(x\)</span>, you want to estimate its probabilities <span class="math">\(p_i\)</span> of being in class <span class="math">\(i\)</span>. However, your algorithm doesn’t directly produce probabilities. Instead it first produces real-valued scores <span class="math">\(y_1,...,y_n\)</span>.From these scores you define the probabilities <span class="math">\(p_i\)</span> using the softmax function.” <span class="citation">(Zheng 2016)</span></li>
<li><p>For more info see <a href="ml-softmax.html">here</a></p></li>
</ul></li>
</ul>
<div id="important-algorithms" class="section level6">
<h6>Important Algorithms</h6>
<ul>
<li><strong>Expectation-Maximization [EM] Algorithm</strong>:provides a general approach to the problem of maximum likelihood parameter estimation in statistical models with latent variables.</li>
</ul>
</div>
<div id="types-of-learning" class="section level6">
<h6>Types of Learning</h6>
<ul>
<li><strong>Active Learning</strong>: we are permitted to actively choose future training data based upon the data that we have previously seen. When we are given this extra flexibility, we can often reduce the need for large quantities of data.</li>
</ul>
</div>
<div id="probabilistic-graphical-models" class="section level6">
<h6>Probabilistic Graphical Models</h6>
<ul>
<li>The relationship between naive Bayes, logistic regression, HMMs, linear-chain CRFs, generative models, and general CRFs from McCallum’s <a href="http://www.research.ed.ac.uk/portal/files/10482724/crftut_fnt.pdf">introduction to CRFs</a>:
<ul>
<li>Simple Generative Model -&gt; Naive Bayes</li>
<li>Simple Discriminative Model -&gt; Logistic Regression</li>
<li>Sequence Generative Model -&gt; HMM</li>
<li>Sequence Discriminative Model -&gt; Linear-chain CRF</li>
<li>General Graphs Generative -&gt; Generative directed Models</li>
<li>General Graphs Discriminative -&gt; General CRFs</li>
</ul></li>
</ul>
<!--}}}-->
</div>
<div id="neural-networks" class="section level6">
<h6>Neural Networks <!--{{{--></h6>
<ul>
<li><strong>Bias Term</strong>:It is useful to think about this term in terms of a line.
<ul>
<li><strong>Line Equation</strong>: <span class="math">\[y = mx + b\]</span></li>
<li>Without a bias term the line equation is y = mx and thus when x is 0 then y HAS TO BE 0. You cant have the line go through the point x=0, y=3. There is nothing you can multiply to zero to make it non-zero. To fix this a bias term is included. b in y=mx+b allows the line to to cut the y axis in a point different than (0,0).</li>
</ul></li>
<li><strong>Learning Rate</strong>: parameter ? controlling how much to change/update the models weights by. We could miss it out, which would be the same as setting it to 1. If we do that, then the weights change a lot whenever there is a wrong answer, which tends to make the network unstable , so that it never settles down. The cost of having a small learning rate is that the weights need to see the inputs more often before they change significantly, so that the network takes longer to learn. However, it will be more stable and resistant to noise (errors) and inaccuracies in the data. <!--}}}--></li>
</ul>
</div>
<div id="deep-learning" class="section level6">
<h6>Deep Learning <!--{{{--></h6>
<ul>
<li><strong>Tensor</strong>: Multidimensional array. <!--}}}--></li>
</ul>
</div>
<div id="nlp" class="section level6">
<h6>NLP <!--{{{--></h6>
<ul>
<li><strong>Language Model</strong>: can be thought of as a function that takes as an input a sequence of words and returns a probability (likelihood) estimate of that sequence <span class="citation">(G. 2015)</span>. The goal is to compute the probability of a sentence or sequence of words <span class="citation">(Jurafsky 2016)</span>. <!--}}}--></li>
</ul>
</div>
<div id="my-own-definitions-could-be-wrong" class="section level6">
<h6>My own definitions (could be wrong) <!--{{{--></h6>
<ul>
<li><strong>kernel</strong>:
<ul>
<li>You only have some data and need more. So transform your current data to a richer feature space where two data points (x1, y1) and (x2, y2) in the original data transform to three points (x1, y1), (x2, y2), and (x1x2, y1y2).</li>
<li>Your problem is not linearly separable and you are using a learning a model that only learns linearly separable problems (like svm). Then transform your data to a feature space where it could be linearly separable. <!--}}}--></li>
</ul></li>
</ul>
</div>
</div>
<div id="relations-among-models" class="section level3">
<h3>Relations Among Models <!--{{{--></h3>
<ul>
<li>Relation between <strong>Perceptron</strong> and <strong>SVMs</strong>. Another powerful and widely used learning algorithm is the <strong>support vector machine (SVM)</strong>, which can be considered as an extension of the <strong>perceptron</strong>.
<ul>
<li>Using the <strong>perceptron</strong> algorithm, we minimized misclassification errors.</li>
<li>However, in <strong>SVMs</strong>, our optimization objective is to maximize the margin.</li>
</ul></li>
<li><strong>Logistic regression versus SVM</strong>: In practical classification tasks, linear logistic regression and linear SVMs often yield very similar results.
<ul>
<li><strong>Logistic regression</strong>: tries to maximize the conditional likelihoods of the training data, which makes it more prone to outliers than SVMs. Logistic regression has the advantage that it is a simpler model that can be implemented more easily.Furthermore, logistic regression models can be easily updated, which is attractive when working with streaming data.</li>
<li>The <strong>SVMs</strong>: mostly care about the points that are closest to the decision boundary (support vectors).</li>
</ul></li>
</ul>
<!--}}}-->
<div class="references">
<h2>REFERENCE</h2>
<p><em>Chain Rule (Probability)</em>. Wikipedia. <a href="https://en.wikipedia.org/wiki/Chain_rule_%28probability%29">https://en.wikipedia.org/wiki/Chain_rule_%28probability%29</a>.</p>
<p>G., Ray M. 2015. <em>How We Made Yelp Search Filters Data Driven</em>. Yelp Engineering Blog. <a href="http://engineeringblog.yelp.com/2015/12/how-we-made-yelp-search-filters-data-driven.html" class="uri">http://engineeringblog.yelp.com/2015/12/how-we-made-yelp-search-filters-data-driven.html</a>.</p>
<p><em>Gradient</em>. 2016. Wikipedia. <a href="https://en.wikipedia.org/wiki/Gradient" class="uri">https://en.wikipedia.org/wiki/Gradient</a>.</p>
<p>Jurafsky, Dan. 2016. “Language Modeling.” <a href="https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf" class="uri">https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf</a>.</p>
<p>“Markov Property.” Wikipedia. <a href="https://en.wikipedia.org/wiki/Markov_property" class="uri">https://en.wikipedia.org/wiki/Markov_property</a>.</p>
<p><em>Maximum Likelihood Estimation</em>. Introduction to Mathematical Statistics Statistics 200. <a href="http://statweb.stanford.edu/~susan/courses/s200/lectures/lect11.pdf" class="uri">http://statweb.stanford.edu/~susan/courses/s200/lectures/lect11.pdf</a>.</p>
<p>Moody, Christopher. 2016. “Word2vec, LDA, and Introducing a New Hibrid Algorithm: Lda2vec.” Slideshare.net. <a href="http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994" class="uri">http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994</a>.</p>
<p>“What’s the Difference Between Gradient Descent and Stochastic Gradient Descent?” 2016. Quora. <a href="https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent" class="uri">https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent</a>.</p>
<p>Wikipedia. “Conditional Probability.” <a href="https://en.wikipedia.org/wiki/Conditional_probability" class="uri">https://en.wikipedia.org/wiki/Conditional_probability</a>.</p>
<p>Zheng, Charles Yang. 2016. <em>What Does the Term “Soft-Max” Mean in the Context of Machine Learning?</em> Quora. <a href="https://www.quora.com/What-does-the-term-soft-max-mean-in-the-context-of-machine-learning" class="uri">https://www.quora.com/What-does-the-term-soft-max-mean-in-the-context-of-machine-learning</a>.</p>
</div>
</div>
</div>


<p>Copyright &copy; 2016 Raul Guerra, Inc. All rights reserved.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
